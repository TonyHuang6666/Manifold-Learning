{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Locality Preserving Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generic problem of linear dimensionality reduction is the following. A set of face image samples {$x_i$} can be represented as an $N$ $\\times$ $M$ matrix $X$ = {$x_1$, $x_2$, ..., $x_M$} where $N$ is the number of pixels in the images and M is the number of samples. Each face image $x_i$ belongs to one of the $C$ face classes {$X_1$, $X_2$, ..., $X_C$}. When a probe $Q$ is the input, the face recognition task is to find its class label in the database.<br>\n",
    "The difference between the Probe $Q$ and the prototype $P$ is $\\Delta$ = $Q$-$P$. From the algirithms of PCA and LDA we can see that the recognition process of the two methods can be described by the same framework: Firstly probe face image $Q$ is the input. We compute the difference $\\Delta$ between $Q$ and each class prototype $P$; then, $\\Delta$ is projected into an image subspace to compute the feature vector; Finally based on the feature vector and the specific distance metric, $\\Delta$ is classified as intrapersonal or interpersonal variations. <br>\n",
    "The two key components of this framework are the image difference $\\Delta$ and its subspace. Especially using a set of theorems, we will show that the two subspaces for PCA and LDA can be computed from the face difference set instead of the original image set. The difference $\\Delta$ can be modeled by three key components: intrinsic difference $I$ that discriminates difference face identities; transformation difference $T$, arising from all kinds of transformations such as expression and illumination changes; noise $N$ which randomly distributes in the face images. $T$ and $N$ are the two components deteriorating cognition performance. Normally, N is of small energy. The main difficulty for face recognition comes from $T$, which can change the face appearance substantially. A successful face recognition algorithm should be able to reducethe energy of $T$ as much as possible without sacrificing much of $I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LPP deemphasizes discriminant information, while discriminant information is important for recognition problem. So the objective function of DLPP is as follows:\n",
    "$$\n",
    "\\frac{\\sum_{c=1}^C \\sum_{i,j=1}^{n_c}(y_i^c-y_j^c)^2 W_{ij}^c}{\\sum_{i,j=1}^C (m_i-m_j)^2 B_{ij}}\n",
    "$$\n",
    "<br>where $C$ is the number of face classes, $n_c$ is the number of samples in the $c$ th class, $y_i^c$ is the $i$ th weight vector in the $c$ th class, $m_i$ and $m_j$ is separately the mean weight vectors for  the $i$ th and $j$ th class, i.e.\n",
    "$$\n",
    "m_i=\\frac{1}{n_i}\\sum_{k=1}^{n_i}y_k^i\n",
    "$$\n",
    "and\n",
    "$$\n",
    "m_j=\\frac{1}{n_j}\\sum_{k=1}^{n_j}y_k^j,\n",
    "$$\n",
    "<br>where $n_i$ and $n_j$ is the number pf samples in the $i$ th and $j$ th class separately. Both $W_{ij}^c$ and $B_{ij}$ are weight matrices.\n",
    "<br>Suppose $a$ is a transformation vector, that is, $Y=a^TX$. By simple algebra formulation, the numerator of objective function can be reduced to:\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{c=1}^C \\sum_{i,j=1}^{n_c} (y_i^c-y_j^c)^2 W_{ij}^c\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{c=1}^C \\sum_{i,j=1}^{n_c} (a^Tx_i^c-a^Tx_j^c)^2 W_{ij}^c\n",
    "$$\n",
    "$$\n",
    "=\\sum_{c=1}^C (\\sum_{i=1}^{n_c} a^T x_i^c D_{ii}^c (x_i^c)^T a- \\sum_{i,j=1}^{n_c} a^T x_i^c W_{ij}^c (x_j^c)^T a)\n",
    "$$\n",
    "$$\n",
    "=\\sum_{c=1}^C a^T X_c (D_c-W_c) X_c^T a\n",
    "$$\n",
    "$$\n",
    "=a^T X(D-W)X^T a\n",
    "$$\n",
    "$$\n",
    "=a^TXLX^Ta\n",
    "$$\n",
    "<br>where $W_c$ is the weight matrix between any two samples in the $c$ th class. And its components can be defined as:\n",
    "$$\n",
    "W_{ij}^c= \\exp(\\frac{-||x_i^c-x_j^c||^2}{t})\n",
    "$$\n",
    "<br>where $t$ is a parameter that can be determined empirically; $D_c$ is a diagonal matrix, and its entries are column (or row since $W_c$ is symmetric) sum of $W_c$. \n",
    "$$\n",
    "D_{ii}^c=\\sum_{j} W_{ij}^c; X=[X_1,X_2,...,X_C];\n",
    "D = \\begin{bmatrix} D_1 & 0 & \\cdots & 0 \\\\ 0 & D_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & D_C \\end{bmatrix};\n",
    "W = \\begin{bmatrix} W_1 & 0 & \\cdots & 0 \\\\ 0 & W_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & W_C \\end{bmatrix};\n",
    "L = D-W\n",
    "$$\n",
    "<br>The denominator of objective function can be reduced to:\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i,j=1}^C (m_i-m_j)^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{i,j=1}^C (\\frac{1}{n_i} \\sum_{k=1}^{n_i} y_k^i - \\frac{1}{n_j} \\sum_{k=1}^{n_j} y_k^j )^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{i,j=1}^C (\\frac{1}{n_i} \\sum_{k=1}^{n_i} a^T x_k^i - \\frac{1}{n_j} \\sum_{k=1}^{n_j} a^T x_k^j )^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{i,j=1}^C [a^T(\\frac{1}{n_i} \\sum_{k=1}^{n_i}  x_k^i) - a^T(\\frac{1}{n_j} \\sum_{k=1}^{n_j} a^T x_k^j )]^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{i,j=1}^C [a^T f_i - a^T f_j]^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\sum_{i=1}^C a^T f_i E_ii f_i^T a - \\sum_{i,j=1}^C a^T f_i B_{ij} f_j^T a= a^T F (E-B) F^T a\n",
    "$$\n",
    "$$\n",
    "=a^TFHF^Ta\n",
    "$$\n",
    "<br>where $F=[f_1,f_2,...,f_c]$ is the mean face in the $i^{th}$ class, i.e. \n",
    "$$\n",
    "f_i=\\frac{1}{n_i} \\sum_{k=1}^{n_i} x_k^i\n",
    "$$.\n",
    "<br>$B$ is the weight matrix between any two classes' mean faces, and its components can be defined as:\n",
    "$$\n",
    "B_{ij}=\\exp(\\frac{-||f_i-f_j||^2}{t}),\n",
    "$$\n",
    "<br>where $t$ is a parameter that can be determined empirically; $E$ is a diagonal matrix, and its entries are column (or row since $B$ is symmetric) sum of $B$. $E_{ii}=\\sum_{j} B_{ij}$; $H$=$E$-$B$.\n",
    "<br>Substitute $a^TXLX^Ta$ and $a^TFHF^Ta$ in $\\frac{\\sum_{c=1}^C \\sum_{i,j=1}^{n_c}(y_i^c-y_j^c)^2 W_{ij}^c}{\\sum_{i,j=1}^C (m_i-m_j)^2 B_{ij}}$, the objective function can be reduced to:\n",
    "$$\n",
    "\\frac{a^TXLX^Ta}{a^TFHF^Ta}\n",
    "$$\n",
    "<br>And DLPP subspace is spanned by a set of vectors $a$, satisfying:\n",
    "$$\n",
    "a = argmin \\frac{a^TXLX^Ta}{a^TFHF^Ta}\n",
    "$$\n",
    "<br>Minimizing the numerator of objective function is an attempt to ensure that if $x_i^c$ and $x_j^c$ are close，then $y_i^c$ and $y_j^c$ are close as well. While maximizing the denominator of it is an attempt to ensure that if $f_i$ and $f_j$ are close but $m_i$ and $m_j$ are far. So minimizing the objective function is minimizing intrapersonal distance and maximizing interpersonal distance.\n",
    "<br>The matrices $XLX^T$ and $FHF^T$ are symmetric and positive semidefinite.The vectors $a_i$ that minimize the objective function are given by minimum eigenvalues solutions to the generalized eigenvalues problem. Let the column vectors $a_0,a_1,...,a_{d-1}$ be the solutions of $a = argmin \\frac{a^TXLX^Ta}{a^TFHF^Ta}$, ordered by their eigenvalues $\\lambda_0 \\leq \\lambda_1 \\leq ... \\leq \\lambda_{d-1}$. Thus the embedding is as follows:\n",
    "$$\n",
    "x_i \\rightarrow y_i = A^T x_i, A=[a_0,a_1,...,a_{d-1}]\n",
    "$$\n",
    "<br>where $y_i$ is a d-dimensional vector, and $A$ is a $N \\times d$ matrix.\n",
    "<br>The numerator of objective function reflects intrapersonal variation while the denominator reflects interpersonal variation. To minimize the objective function is to minimize intrapersonal variation and maximize interpersonal valuation. In terms of difference model, minimizing intrapersonal variation is to reduce noise and transformation difference while maximizing interpersonal variation is to enhance intrinsic difference. In addition, denominator uses the mean values of classes to reduce much of the transformation difference in between-class distance. The most noise can be removed by PCA. So the face data can firstly be projected to PCA subspace and select the principal components to remove most of the noise, and then be projected to DLPP subspace to reduce transformation difference and enhance the intrinsic difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 局部判别保持投影"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于线性降维的面部识别问题的一般形式如下：一组面部图像样本 {$x_i$} 可以表示为一个 $N$ $\\times$ $M$ 矩阵 $X$ = {$x_1$, $x_2$, ..., $x_M$}，其中 $N$ 是图像中的像素数量，$M$ 是样本数量。每个面部图像 $x_i$ 属于 $C$ 个面部类别 {$X_1$, $X_2$, ..., $X_C$} 中的一个。当探测器 $Q$ 是输入时，面部识别任务是在数据库中找到其类别标签。<br>\n",
    "\n",
    "探测器 $Q$ 和原型 $P$ 之间的差异是 $\\Delta$ = $Q$-$P$。从 PCA 和 LDA 的算法中我们可以看到，两种方法的识别过程可以用相同的框架来描述：首先，探测器面部图像 $Q$ 是输入。我们计算 $Q$ 和每个类别原型 $P$ 之间的差异 $\\Delta$；然后，$\\Delta$ 被投影到一个图像子空间来计算特征向量；最后，基于特征向量和特定的距离度量，$\\Delta$ 被分类为个体内或个体间的变异。<br>\n",
    "\n",
    "这个框架的两个关键组成部分是图像差异 $\\Delta$ 和它的子空间。特别是使用一组定理，我们将展示 PCA 和 LDA 的两个子空间可以从面部差异集合而不是原始图像集合中计算出来。差异 $\\Delta$ 可以由三个关键组成部分建模：内在差异 $I$，用于区分不同的面部身份；变换差异 $T$，由各种变换（如表情和光照变化）产生；噪声 $N$，在面部图像中随机分布。$T$ 和 $N$ 是两个降低识别性能的组成部分。通常，N 的能量很小。面部识别的主要困难来自 $T$，它可以显著改变面部外观。一个成功的面部识别算法应该能够尽可能地减少 $T$ 的能量，而不牺牲太多的 $I$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LPP（局部保持投影）减弱了判别信息，而判别信息对于识别问题很重要。因此，DLPP 的目标函数如下：\n",
    "$$\n",
    "\\frac{\\sum_{c=1}^C \\sum_{i,j=1}^{n_c}(y_i^c-y_j^c)^2 W_{ij}^c}{\\sum_{i,j=1}^C (m_i-m_j)^2 B_{ij}}\n",
    "$$\n",
    "<br>其中 $C$ 是人脸类别的数量，$n_c$ 是第 $c$ 类中的样本数，$y_i^c$ 是第 $c$ 类中第 $i$ 个权重向量，$m_i$ 和 $m_j$ 分别是第 $i$ 类和第 $j$ 类的均值权重向量，即\n",
    "$$\n",
    "m_i=\\frac{1}{n_i}\\sum_{k=1}^{n_i}y_k^i\n",
    "$$\n",
    "和\n",
    "$$\n",
    "m_j=\\frac{1}{n_j}\\sum_{k=1}^{n_j}y_k^j,\n",
    "$$\n",
    "<br>其中 $n_i$ 和 $n_j$ 分别是第 $i$ 类和第 $j$ 类的样本数。$W_{ij}^c$ 和 $B_{ij}$ 都是权重矩阵。\n",
    "<br>假设 $a$ 是一个变换向量，即 $Y=a^TX$。通过简单的代数推导，目标函数的分子可以简化为：\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{c=1}^C \\sum_{i,j=1}^{n_c} (y_i^c-y_j^c)^2 W_{ij}^c\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{c=1}^C \\sum_{i,j=1}^{n_c} (a^Tx_i^c-a^Tx_j^c)^2 W_{ij}^c\n",
    "$$\n",
    "$$\n",
    "=\\sum_{c=1}^C (\\sum_{i=1}^{n_c} a^T x_i^c D_{ii}^c (x_i^c)^T a- \\sum_{i,j=1}^{n_c} a^T x_i^c W_{ij}^c (x_j^c)^T a)\n",
    "$$\n",
    "$$\n",
    "=\\sum_{c=1}^C a^T X_c (D_c-W_c) X_c^T a\n",
    "$$\n",
    "$$\n",
    "=a^T X(D-W)X^T a\n",
    "$$\n",
    "$$\n",
    "=a^TXLX^Ta\n",
    "$$\n",
    "<br>其中 $W_c$ 是第 $c$ 类中任意两个样本之间的权重矩阵。其组成可以定义为：\n",
    "$$\n",
    "W_{ij}^c= \\exp(\\frac{-||x_i^c-x_j^c||^2}{t})\n",
    "$$\n",
    "<br>其中 $t$ 是一个经验确定的参数；$D_c$ 是对角矩阵，其元素是 $W_c$ 的列和（或行，因为 $W_c$ 是对称的）。\n",
    "$$\n",
    "D_{ii}^c=\\sum_{j} W_{ij}^c; X=[X_1,X_2,...,X_C];\n",
    "D = \\begin{bmatrix} D_1 & 0 & \\cdots & 0 \\\\ 0 & D_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & D_C \\end{bmatrix};\n",
    "W = \\begin{bmatrix} W_1 & 0 & \\cdots & 0 \\\\ 0 & W_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & W_C \\end{bmatrix};\n",
    "L = D-W\n",
    "$$\n",
    "<br>目标函数的分母可以简化为：\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i,j=1}^C (m_i-m_j)^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{i,j=1}^C (\\frac{1}{n_i} \\sum_{k=1}^{n_i} y_k^i - \\frac{1}{n_j} \\sum_{k=1}^{n_j} y_k^j )^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{i,j=1}^C (\\frac{1}{n_i} \\sum_{k=1}^{n_i} a^T x_k^i - \\frac{1}{n_j} \\sum_{k=1}^{n_j} a^T x_k^j )^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{i,j=1}^C [a^T(\\frac{1}{n_i} \\sum_{k=1}^{n_i}  x_k^i) - a^T(\\frac{1}{n_j} \\sum_{k=1}^{n_j} a^T x_k^j )]^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\frac{1}{2} \\sum_{i,j=1}^C [a^T f_i - a^T f_j]^2 B_{ij}\n",
    "$$\n",
    "$$\n",
    "=\\sum_{i=1}^C a^T f_i E_ii f_i^T a - \\sum_{i,j=1}^C a^T f_i B_{ij} f_j^T a= a^T F (E-B) F^T a\n",
    "$$\n",
    "$$\n",
    "=a^TFHF^Ta\n",
    "$$\n",
    "<br>其中 $F=[f_1,f_2,...,f_c]$ 是第 $i^{th}$ 类中的平均面部，即\n",
    "$$\n",
    "f_i=\\frac{1}{n_i} \\sum_{k=1}^{n_i} x_k^i\n",
    "$$.\n",
    "<br>$B$ 是任意两个类的平均面部之间的权重矩阵，其组成可以定义为：\n",
    "$$\n",
    "B_{ij}=\\exp(\\frac{-||f_i-f_j||^2}{t}),\n",
    "$$\n",
    "<br>其中 $t$ 是一个经验确定的参数；$E$ 是对角矩阵，其元素是 $B$ 的列和（或行，因为 $B$ 是对称的）。$E_{ii}=\\sum_{j} B_{ij}$；$H$=$E$-$B$。\n",
    "<br>将 $a^TXLX^Ta$ 和 $a^TFHF^Ta$ 代入 $\\frac{\\sum_{c=1}^C \\sum_{i,j=1}^{n_c}(y_i^c-y_j^c)^2 W_{ij}^c}{\\sum_{i,j=1}^C (m_i-m_j)^2 B_{ij}}$，目标函数可以简化为：\n",
    "$$\n",
    "\\frac{a^TXLX^Ta}{a^TFHF^Ta}\n",
    "$$\n",
    "<br>DLPP 子空间由一组向量 $a$ 张成，满足：\n",
    "$$\n",
    "a = argmin \\frac{a^TXLX^Ta}{a^TFHF^Ta}\n",
    "$$\n",
    "<br>最小化目标函数的分子是为了确保如果 $x_i^c$ 和 $x_j^c$ 接近，那么 $y_i^c$ 和 $y_j^c$ 也接近。而最大化目标函数的分母是为了确保如果 $f_i$ 和 $f_j$ 接近但 $m_i$ 和 $m_j$ 远离。因此，最小化目标函数是最小化个体内部距离和最大化个体间距离。\n",
    "<br>矩阵 $XLX^T$ 和 $FHF^T$ 是对称且半正定的。最小化目标函数的向量 $a_i$ 由广义特征值问题的最小特征值解给出。设列向量 $a_0,a_1,...,a_{d-1}$ 是 $a = \\text{argmin} \\frac{a^TXLX^Ta}{a^TFHF^Ta}$ 的解，按其特征值 $\\lambda_0 \\leq \\lambda_1 \\leq ... \\leq \\lambda_{d-1}$ 排序。因此，嵌入是：\n",
    "$$\n",
    "x_i \\rightarrow y_i = A^T x_i, A=[a_0,a_1,...,a_{d-1}]\n",
    "$$\n",
    "<br>其中 $y_i$ 是一个 $d$ 维向量，$A$ 是一个 $N \\times d$ 矩阵。\n",
    "<br>目标函数的分子反映个体内部变化，而分母反映个体间变化。最小化目标函数是为了最小化个体内部变化和最大化个体间变化。在差异模型方面，最小化个体内部变化是为了减少噪音和变换差异，而最大化个体间变化是为了增强固有差异。此外，分母使用类的均值来减少类间距离中的变换差异。PCA 可以去除大部分噪音，因此可以首先将面部数据投影到 PCA 子空间，并选择主成分来去除大部分噪音，然后再投影到 DLPP 子空间，以减少变换差异并增强固有差异。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
