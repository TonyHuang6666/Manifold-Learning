{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库函数\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.linalg import eigs\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################LPP算法函数######################################\n",
    "#####################计算自适应epsilon graph####################\n",
    "def compute_adaptive_neighbors(Data):\n",
    "    n = Data.shape[1]  # 样本点的数量\n",
    "    distances = np.sqrt(np.sum((Data.T[:, :, None] - Data.T[:, :, None].T) ** 2, axis=1)) \n",
    "    sorted_distances = np.sort(distances, axis=1)  # 对距离矩阵的每一行进行排序\n",
    "    np.savetxt('sorted_distances.csv', sorted_distances, delimiter=',')\n",
    "    adaptive_neighbors = np.zeros((n, 1))\n",
    "    # 对每行距离进行插值和求导\n",
    "    for i in range(n):\n",
    "        # 对距离进行插值，生成连续的函数\n",
    "        f = interp1d(np.arange(n), sorted_distances[i], kind='linear')\n",
    "        #coefficients = np.polyfit(np.arange(n), sorted_distances[i], deg=5)  # 使用6次多项式拟合\n",
    "        #f = np.poly1d(coefficients)  # 构建多项式函数\n",
    "        # 求导\n",
    "        df = np.gradient(f(np.arange(n)))  # 计算函数的导数\n",
    "        # 寻找导数为1的位置\n",
    "        idx = np.where(df <= 1)[0][0]\n",
    "        # 将索引保存为每个数据点的邻居数量\n",
    "        adaptive_neighbors[i] = idx\n",
    "    return adaptive_neighbors, sorted_distances, distances\n",
    "\n",
    "# 根据adaptive_neighbors中每一行的邻居数量和sorted_distances对每个数据点构建epsilon graph\n",
    "def adaptive_epsilon_graph(Data, method, k):\n",
    "    adaptive_neighbors, sorted_distances, distances = compute_adaptive_neighbors(Data)\n",
    "    n = Data.shape[1]  \n",
    "    adaptive_epsilon_adjacency_matrix = np.zeros((n, n)) \n",
    "    if method == \"Adaptive epsilon\": # 计算自适应epsilon graph，不管k值\n",
    "        for i in range(n):\n",
    "            indices = np.argsort(sorted_distances[i])[:int(adaptive_neighbors[i])]\n",
    "            adaptive_epsilon_adjacency_matrix[i, indices] = 1\n",
    "            adaptive_epsilon_adjacency_matrix[indices, i] = 1\n",
    "        return adaptive_epsilon_adjacency_matrix, distances\n",
    "    elif method == \"KNN + Adaptive epsilon\": # 以k值为邻居数的阈值，小于k值的邻居数使用adaptive epsilon graph，大于k值的邻居数使用knn graph\n",
    "        for i in range(n):\n",
    "            if adaptive_neighbors[i] <= k:\n",
    "                indices = np.argsort(sorted_distances[i])[:int(adaptive_neighbors[i])]\n",
    "                adaptive_epsilon_adjacency_matrix[i, indices] = 1\n",
    "                adaptive_epsilon_adjacency_matrix[indices, i] = 1\n",
    "            else:\n",
    "                indices = np.argsort(distances[i])[:k]\n",
    "                adaptive_epsilon_adjacency_matrix[i, indices] = 1\n",
    "                adaptive_epsilon_adjacency_matrix[indices, i] = 1\n",
    "        return adaptive_epsilon_adjacency_matrix, distances\n",
    "\n",
    "\n",
    "#####################计算自适应epsilon graph####################\n",
    "\n",
    "#####################计算经典knn graph####################\n",
    "def knn_graph(Data, method, k):\n",
    "    n = Data.shape[1]  \n",
    "    knn_adjacency_matrix = np.zeros((n, n))  \n",
    "    distances = np.sqrt(np.sum((Data.T[:, :, None] - Data.T[:, :, None].T) ** 2, axis=1))\n",
    "    if method == \"Average-KNN-distances-based epsilon\":\n",
    "        return distances\n",
    "    indices = np.argsort(distances, axis=1)[:, 1:k+1]\n",
    "    for i in range(n):\n",
    "        knn_adjacency_matrix[i, indices[i]] = 1\n",
    "        knn_adjacency_matrix[indices[i], i] = 1\n",
    "    return knn_adjacency_matrix, distances\n",
    "#####################计算经典knn graph####################\n",
    "\n",
    "#####################计算epsilon graph####################\n",
    "def compute_avg_radius(n, distances): \n",
    "    radius = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        avg_radius = np.mean(distances[:, i])  # 修改计算每个数据点的平均邻域半径的方式\n",
    "        radius[i] = avg_radius\n",
    "    return radius\n",
    "\n",
    "def compute_knn_average_radius(sorted_distances, k):\n",
    "    avg_knn_distances = np.mean(sorted_distances[:, 1:k+1], axis=1)  # 计算每个数据点的前k个距离的平均值作为半径\n",
    "    return avg_knn_distances\n",
    "#####################计算epsilon graph####################\n",
    "\n",
    "def compute_neighborhood_matrix(Data, method, k):\n",
    "    n = Data.shape[1]  # 获取样本点的数量\n",
    "    if method == \"KNN\":\n",
    "        knn_adjacency_matrix, distances = knn_graph(Data, method, k)\n",
    "        return knn_adjacency_matrix, distances\n",
    "    elif method == \"Adaptive epsilon\":\n",
    "        adaptive_epsilon_adjacency_matrix, distances = adaptive_epsilon_graph(Data, method, k)\n",
    "        return adaptive_epsilon_adjacency_matrix, distances\n",
    "    elif method == \"Average-KNN-distances-based epsilon\":\n",
    "        epsilon_adjacency_matrix = np.zeros((n, n))\n",
    "        distances = knn_graph(Data, method, k)\n",
    "        sorted_distances = np.sort(distances, axis=1)\n",
    "        radius = compute_knn_average_radius(sorted_distances, k)\n",
    "        for i in range(n):\n",
    "            neighbors = np.where(distances[:, i] <= radius[i])[0]  \n",
    "            epsilon_adjacency_matrix[i, neighbors] = 1\n",
    "            epsilon_adjacency_matrix[neighbors, i] = 1\n",
    "        return epsilon_adjacency_matrix, distances\n",
    "    elif method == \"KNN + Adaptive epsilon\":\n",
    "        epsilon_adjacency_matrix, distances = adaptive_epsilon_graph(Data, method, k)\n",
    "        return epsilon_adjacency_matrix, distances\n",
    "\n",
    "def construct_weight_matrix(Data, method, k,t):\n",
    "    n = Data.shape[1]  \n",
    "    Weight_matrix = np.zeros((n, n))\n",
    "    adjacency_matrix, distances = compute_neighborhood_matrix(Data, method, k)\n",
    "    similarity_matrix = np.exp(-distances ** 2 / t)\n",
    "    i_indices, j_indices = np.where(adjacency_matrix == 1)\n",
    "    Weight_matrix[i_indices, j_indices] = similarity_matrix[i_indices, j_indices]\n",
    "    Weight_matrix[j_indices, i_indices] = similarity_matrix[i_indices, j_indices]\n",
    "    #Weight_matrix += np.exp(-distances ** 2 / t) 如果修正那么有用那对邻域的改进还有什么意义？？\n",
    "    return Weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, n_components):\n",
    "    # 计算数据矩阵的均值\n",
    "    mean = np.mean(X, axis=0)\n",
    "    # 中心化数据矩阵\n",
    "    X_centered = X - mean\n",
    "    # 计算数据矩阵的协方差矩阵\n",
    "    covariance_matrix = np.cov(X_centered, rowvar=False)\n",
    "    # 计算协方差矩阵的特征值和特征向量\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    # 对特征向量按特征值从大到小排序\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    # 选取前n_components个特征向量\n",
    "    principal_components = sorted_eigenvectors[:, :n_components]\n",
    "    # 返回投影矩阵和均值向量\n",
    "    return principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLDA算法函数\n",
    "\n",
    "# 计算每个类别的均值矩阵\n",
    "def compute_classes_mean_matrix(train_data, train_labels):\n",
    "    num_classes = len(np.unique(train_labels))  # 类别数量\n",
    "    print(\"类别数量：\", num_classes)\n",
    "    num_samples_per_class = train_data.shape[0] // num_classes  # 每个类别的样本数\n",
    "    print(\"每个类别的样本数：\", num_samples_per_class)\n",
    "    num_features = train_data.shape[1]  # 每个样本的特征维度\n",
    "    print(\"每个样本的特征维度：\", num_features)\n",
    "    means = np.zeros((num_classes, num_features))  # 存储每个类别的均值矩阵\n",
    "    for i in range(1, num_classes + 1):  # 遍历每个类别\n",
    "        temp_indices = np.where(train_labels == i)[0]  # 获取当前类别的训练数据索引\n",
    "        temp_sum = np.sum(train_data[temp_indices], axis=0)  # 计算当前类别的特征值总和\n",
    "        means[i-1] = temp_sum / num_samples_per_class  # 计算当前类别的均值\n",
    "    return means  # 返回每个类别的均值矩阵\n",
    "\n",
    "# 计算所有类别的整体均值矩阵\n",
    "def compute_overall_mean_matrix(classes_means):\n",
    "    overall_mean = np.mean(classes_means, axis=0)  # 计算所有类别的整体均值\n",
    "    return overall_mean.reshape(-1, 1)  # 返回整体均值矩阵（转置）\n",
    "\n",
    "# 计算中心类别矩阵\n",
    "def compute_center_class_matrix(train_data, train_labels, classes_means):\n",
    "    Z = np.zeros_like(train_data)  # 初始化中心类别矩阵\n",
    "    \n",
    "    for i in range(train_data.shape[0]):  # 遍历训练数据\n",
    "        class_index = int(train_labels[i]) - 1  # 获取当前样本所属类别的索引\n",
    "        Z[i] = train_data[i] - classes_means[class_index]  # 计算中心类别矩阵\n",
    "        \n",
    "    return Z  # 返回中心类别矩阵\n",
    "\n",
    "# 计算类间散布矩阵\n",
    "def compute_between_class_scatter_matrix(classes_means, overall_mean):\n",
    "    n = 5  # 训练集与测试集的比例\n",
    "    Sb = np.zeros((classes_means.shape[1], classes_means.shape[1]))  # 初始化类间散布矩阵\n",
    "    for i in range(classes_means.shape[0]):  # 遍历每个类别的均值矩阵\n",
    "        Sb = np.add(Sb, n * ((classes_means[i] - overall_mean) * (classes_means[i] - overall_mean).T))  # 计算类间散布矩阵\n",
    "    return Sb  # 返回类间散布矩阵\n",
    "\n",
    "# 计算类内散布矩阵\n",
    "def compute_class_scatter_matrix(Z):\n",
    "    Sw = np.dot(Z.T, Z)  # 计算类内散布矩阵\n",
    "    return Sw  # 返回类内散布矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowEigenface(eigenfaces, faceshape):\n",
    "    # 显示前16个特征脸\n",
    "    fig, axes = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(8, 10))\n",
    "    for i in range(16):\n",
    "        axes[i%4][i//4].imshow(eigenfaces[:, i].reshape(faceshape), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个类别的权重矩阵，度矩阵和拉普拉斯矩阵\n",
    "def DLPP_LPP(train_data, method, k, t):\n",
    "    train_data = train_data.T\n",
    "    Weight_matrix = construct_weight_matrix(train_data, method, k, t)\n",
    "    Degree_matrix = np.diag(np.sum(Weight_matrix, axis=1))\n",
    "    Laplacian_matrix = Degree_matrix - Weight_matrix\n",
    "    return Laplacian_matrix, train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个类别的均值矩阵\n",
    "def DLPP_MLDA(train_data, train_labels):\n",
    "    classes_means = compute_classes_mean_matrix(train_data, train_labels)\n",
    "    return classes_means.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPP(Data, d, method, k, t):\n",
    "    Data = Data.T\n",
    "    Weight_matrix = construct_weight_matrix(Data, method, k, t)\n",
    "    Degree_matrix = np.diag(np.sum(Weight_matrix, axis=1))\n",
    "    Laplacian_matrix = Degree_matrix - Weight_matrix\n",
    "    print(\"LPP拉普拉斯矩阵形状：\", Laplacian_matrix.shape)\n",
    "    objective_value = np.dot(np.dot(Data, Laplacian_matrix), Data.T)  # 计算目标函数\n",
    "    eigenvalues, eigenvectors = eigs(objective_value, k=d+1)\n",
    "    sorted_indices = np.argsort(eigenvalues.real)\n",
    "    selected_indices = sorted_indices[1:d + 1]\n",
    "    selected_eigenvectors = eigenvectors.real[:, selected_indices]\n",
    "    return selected_eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def MLDA(train_data, train_labels, faceshape, d):\n",
    "    # 计算每个类别的均值矩阵\n",
    "    classes_means = compute_classes_mean_matrix(train_data, train_labels)\n",
    "    \n",
    "    print('classes_means形状：', classes_means.shape)\n",
    "\n",
    "    # 计算所有类别的整体均值矩阵\n",
    "    overall_mean = compute_overall_mean_matrix(classes_means)\n",
    "    print('overall_mean形状：', overall_mean.shape)\n",
    "\n",
    "    # 计算中心类别矩阵\n",
    "    Z = compute_center_class_matrix(train_data, train_labels, classes_means)\n",
    "    print('Z形状：', Z.shape)\n",
    "\n",
    "    # 计算类间散布矩阵\n",
    "    Sb = compute_between_class_scatter_matrix(classes_means, overall_mean)\n",
    "    print('Sb形状：', Sb.shape)\n",
    "\n",
    "    # 计算类内散布矩阵\n",
    "    Sw = compute_class_scatter_matrix(Z)\n",
    "    print('Sw形状：', Sw.shape)\n",
    "    \n",
    "    W_value = np.dot(Sw.T, Sb)  # 计算投影矩阵W\n",
    "    print(W_value.shape)  # 输出投影矩阵W的形状\n",
    "\n",
    "    # 计算广义特征值问题的特征值和特征向量，提取前d个最大特征值对应的特征向量\n",
    "    eigen_values, eigen_vectors = scipy.linalg.eigh(W_value, eigvals=((faceshape[0] * faceshape[1]-d),(faceshape[0] * faceshape[1]-1)))  # 计算特征值和特征向量\n",
    "    print('Done LDA selected eigenvectors computing')  # 输出提示信息\n",
    "    print('LDA特征脸形状：', eigen_vectors.shape)\n",
    "    ShowEigenface(eigen_vectors, faceshape)\n",
    "    return eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLPP(train_data, train_labels, d, lpp_method, k, t):\n",
    "    # Step 1: 使用MLDA进行特征提取\n",
    "    F = DLPP_MLDA(train_data, train_labels)\n",
    "    # Step 2: 使用LPP进行特征提取\n",
    "    L, X = DLPP_LPP(train_data, lpp_method, k, t)\n",
    "    # Step 3: 计算权重矩阵B\n",
    "    num_classes = len(np.unique(train_labels))  # 计算训练集中的类别数\n",
    "    B = np.zeros((num_classes, num_classes))  # 初始化权重矩阵B\n",
    "    # 遍历每对类别，计算其对应的权重\n",
    "    for i in range(num_classes):  # 遍历每个类别\n",
    "        for j in range(num_classes):  # 再次遍历每个类别\n",
    "            if i != j:  # 如果类别不相同\n",
    "                fi = F[:,i]  # 获取第i个类别的平均脸\n",
    "                fj = F[:,j]  # 获取第j个类别的平均脸\n",
    "                # 计算第i类别和第j类别平均脸之间的欧氏距离，并将其应用于高斯核函数，计算权重\n",
    "                B[i, j] = np.exp(-np.linalg.norm(fi - fj) ** 2 / t)\n",
    "    # Step 4: 计算E和H矩阵\n",
    "    E = np.diag(np.sum(B, axis=1))\n",
    "    H = E - B\n",
    "\n",
    "    # Step 5: 分式\n",
    "    denominator = np.dot(np.dot(F, H), F.T) + 1e-10  # 添加一个微小的非零值，以避免除以零\n",
    "    numerator = np.dot(np.dot(X, L), X.T)\n",
    "    objective_value = numerator / denominator\n",
    "\n",
    "    # Step 6: 求解广义特征值问题的特征值和特征向量\n",
    "    # eigs用于稀疏矩阵，eigh用于稠密矩阵\n",
    "    eigenvalues, eigenvectors = eigs(objective_value, k=d+1)\n",
    "    sorted_indices = np.argsort(eigenvalues.real)\n",
    "    selected_indices = sorted_indices[1:d + 1]  \n",
    "    selected_eigenvectors = eigenvectors.real[:, selected_indices] \n",
    "    return F, L, B, objective_value, selected_eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库函数\n",
    "import numpy as np\n",
    "from os import listdir, path, remove\n",
    "from cv2 import imread, resize, INTER_AREA, IMREAD_GRAYSCALE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "# 读取数据集\n",
    "def read_ORL_UMIST_yalefaces_images(dataset_dir, target_size=None):\n",
    "    data = []  # 存储图像数据的列表\n",
    "    labels = []  # 存储标签的列表\n",
    "    faceshape = [] # 存储图像形状\n",
    "    for class_dir in listdir(dataset_dir):  # 遍历数据集文件夹中的文件夹（每个文件夹代表一个类别）\n",
    "        class_path = path.join(dataset_dir, class_dir)  # 类别文件夹路径\n",
    "        for file_name in listdir(class_path):  # 遍历每个类别文件夹中的图像文件\n",
    "            file_path = path.join(class_path, file_name)  # 图像文件路径\n",
    "            if file_name.endswith('.gif'):  # 如果文件格式为.gif\n",
    "                # 使用PIL库打开.gif文件\n",
    "                with Image.open(file_path) as img:\n",
    "                    # 转换为灰度图像\n",
    "                    img = img.convert('L')\n",
    "                    # 保存为.pgm格式\n",
    "                    pgm_file_path = file_path.replace('.gif', '.pgm')\n",
    "                    img.save(pgm_file_path)\n",
    "                    # 读取.pgm文件并继续后续处理\n",
    "                    img = imread(pgm_file_path, IMREAD_GRAYSCALE)\n",
    "                    # 如果指定了目标尺寸，则缩放图像\n",
    "                    if target_size is not None:\n",
    "                        img = resize(img, target_size, interpolation=INTER_AREA)\n",
    "                    remove(pgm_file_path)  # 删除临时保存的.pgm文件\n",
    "            elif file_name.endswith('.tif'):  # 如果文件格式为.tif\n",
    "                # 使用PIL库打开.tif文件\n",
    "                with Image.open(file_path) as img:\n",
    "                    # 转换为灰度图像\n",
    "                    img = img.convert('L')\n",
    "                    # 保存为.pgm格式\n",
    "                    pgm_file_path = file_path.replace('.tif', '.pgm')\n",
    "                    img.save(pgm_file_path)\n",
    "                    # 读取.pgm文件并继续后续处理\n",
    "                    img = imread(pgm_file_path, IMREAD_GRAYSCALE)\n",
    "                    # 如果指定了目标尺寸，则缩放图像\n",
    "                    if target_size is not None:\n",
    "                        img = resize(img, target_size, interpolation=INTER_AREA)\n",
    "                    remove(pgm_file_path)        \n",
    "            else:\n",
    "                img = imread(file_path, IMREAD_GRAYSCALE)  # 读取灰度图像\n",
    "                # 如果指定了目标尺寸，则缩放图像\n",
    "                if target_size is not None:\n",
    "                    img = resize(img, target_size, interpolation=INTER_AREA)\n",
    "            # 读取第一张灰度图像的大小作为图片形状\n",
    "            if not faceshape:\n",
    "                faceshape = img.shape\n",
    "            data.append(img.flatten())  # 将图像展平并添加到数据列表中\n",
    "            labels.append(int(class_dir))  # 将类别标签添加到标签列表中\n",
    "    return np.array(data), np.array(labels).reshape(-1, 1), faceshape  # 返回图像数据和标签\n",
    "\n",
    "# 训练集和测试集划分(按顺序划分)\n",
    "def train_test_split(data, labels, train_test_split_ratio):\n",
    "    num_samples = data.shape[0]  # 总样本数\n",
    "    num_classes = len(np.unique(labels))  # 类别数\n",
    "    train_samples_per_class = int(train_test_split_ratio * num_samples / num_classes)  # 每个类别的训练样本数\n",
    "    \n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    for i in range(1, num_classes + 1):  # 对每个类别\n",
    "        class_indices = np.where(labels == i)[0]  # 获取当前类别的索引\n",
    "        train_indices.extend(class_indices[:train_samples_per_class])  # 将前面部分作为训练集\n",
    "        test_indices.extend(class_indices[train_samples_per_class:])  # 将后面部分作为测试集\n",
    "    \n",
    "    train_data = data[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "    test_data = data[test_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "def split(data, labels, train_test_split_ratio, state):\n",
    "    classes = np.unique(labels)\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    for i in range(min(classes), max(classes) + 1):  # Iterate over each class\n",
    "        class_indices = np.where(labels == i)[0]  # Get indices for the current class\n",
    "        # Split the samples of the current class into training and testing sets\n",
    "        train_idx, test_idx = train_test_split(class_indices, test_size=(1 - train_test_split_ratio), train_size=train_test_split_ratio, random_state=state)\n",
    "        train_indices.extend(train_idx)\n",
    "        test_indices.extend(test_idx)\n",
    "\n",
    "    train_data = data[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "    test_data = data[test_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(i, train_labels, test_labels, query, weight_matrix):\n",
    "    query = query.reshape(-1, 1)\n",
    "    # 计算测试图像权重与数据集中每个人脸权重的欧氏距离\n",
    "    euclidean_distances = np.linalg.norm(weight_matrix - query, axis=0)\n",
    "    # 找到最佳匹配的人脸\n",
    "    best_match_index = np.argmin(euclidean_distances)\n",
    "    #判断是否匹配正确\n",
    "    flag = True\n",
    "    if train_labels[best_match_index] == test_labels[i]:\n",
    "        flag = True\n",
    "    else:\n",
    "        flag = False\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据形状: (165, 1024)\n",
      "训练集X形状： (75, 1024)\n",
      "训练集标签形状： (75, 1)\n",
      "测试集X形状： (90, 1024)\n",
      "测试集标签形状： (90, 1)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.datasets import load_digits\n",
    "#digits = load_digits()\n",
    "#data = digits.data\n",
    "#labels = digits.target\n",
    "#images = digits.images\n",
    "\n",
    "d = 40\n",
    "k = 5\n",
    "t = 161000\n",
    "#t=200000\n",
    "lpp_method = 'Adaptive epsilon' # 'knn'或'epsilon'\n",
    "\n",
    "dataset = \"yalefaces\"\n",
    "data, labels, faceshape = read_ORL_UMIST_yalefaces_images(dataset, target_size=(32, 32))\n",
    "print(\"数据形状:\", data.shape)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = split(data, labels, 0.5, 0)\n",
    "print(\"训练集X形状：\", train_data.shape)\n",
    "print(\"训练集标签形状：\", train_labels.shape)\n",
    "print(\"测试集X形状：\", test_data.shape)\n",
    "print(\"测试集标签形状：\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别数量： 15\n",
      "每个类别的样本数： 5\n",
      "每个样本的特征维度： 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tony-Huang\\AppData\\Local\\Temp\\ipykernel_28468\\1017425255.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  indices = np.argsort(sorted_distances[i])[:int(adaptive_neighbors[i])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dlpp_eigenvectors \u001b[38;5;241m=\u001b[39m DLPP(train_data, train_labels, d, lpp_method, k, t)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDLPP变换矩阵A形状:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdlpp_eigenvectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m      3\u001b[0m dlpp_weight_matrix \u001b[38;5;241m=\u001b[39m dlpp_eigenvectors\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m train_data\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDLPP子空间Y形状:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dlpp_weight_matrix\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "dlpp_eigenvectors = DLPP(train_data, train_labels, d, lpp_method, k, t)\n",
    "print(\"DLPP变换矩阵A形状:\", dlpp_eigenvectors.shape)\n",
    "dlpp_weight_matrix = dlpp_eigenvectors.T @ train_data.T\n",
    "print(\"DLPP子空间Y形状:\", dlpp_weight_matrix.shape)\n",
    "\n",
    "dlpp_test_data = dlpp_eigenvectors.T @ test_data.T\n",
    "print(\"DLPP子空间的测试集形状:\", dlpp_test_data.shape)\n",
    "\n",
    "ShowEigenface(dlpp_eigenvectors, faceshape)\n",
    "\n",
    "# 识别率统计\n",
    "wrong_times = 0\n",
    "right_times = 0\n",
    "for i in range(test_data.shape[0]):\n",
    "    flag = test_image(i, train_labels, test_labels, dlpp_test_data[:,i], dlpp_weight_matrix)\n",
    "    if flag:\n",
    "                right_times += 1\n",
    "    else:\n",
    "        wrong_times += 1\n",
    "rate = right_times / test_data.shape[0]\n",
    "print(\"识别率：\", rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpp_eigenvectors = LPP(train_data, d, lpp_method, k, t)\n",
    "lpp_weight_matrix = lpp_eigenvectors.T @ train_data.T\n",
    "print(\"LPP变换矩阵A形状:\", lpp_eigenvectors.shape)\n",
    "print(\"LPP子空间Y形状:\", lpp_weight_matrix.shape)\n",
    "lpp_test_data = lpp_eigenvectors.T @ test_data.T\n",
    "ShowEigenface(lpp_eigenvectors, faceshape)\n",
    "\n",
    "\n",
    "# 识别率统计\n",
    "wrong_times = 0\n",
    "right_times = 0\n",
    "for i in range(test_data.shape[0]):\n",
    "    flag = test_image(i, train_labels, test_labels, lpp_test_data[:,i], lpp_weight_matrix)\n",
    "    if flag:\n",
    "                right_times += 1\n",
    "    else:\n",
    "        wrong_times += 1\n",
    "rate = right_times / test_data.shape[0]\n",
    "print(\"识别率：\", rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_by_pca = PCA(train_data, d)\n",
    "print(\"PCA训练数据主成分形状:\", data_by_pca.shape)\n",
    "test_data_by_pca = PCA(test_data.T, d)\n",
    "print(\"PCA测试数据主成分形状:\", test_data_by_pca.shape)\n",
    "\n",
    "\"\"\"\n",
    "# 识别率统计\n",
    "wrong_times = 0\n",
    "right_times = 0\n",
    "for i in range(test_data.shape[0]):\n",
    "    flag = test_image(i, train_labels, test_labels, test_data_by_pca[i], data_by_pca.T)\n",
    "    if flag:\n",
    "                right_times += 1\n",
    "    else:\n",
    "        wrong_times += 1\n",
    "rate = right_times / test_data.shape[0]\n",
    "print(\"识别率：\", rate)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vectors = MLDA(train_data, train_labels, faceshape, d)\n",
    "print(\"MLDA变换矩阵A形状:\", eigen_vectors.shape)\n",
    "ShowEigenface(eigen_vectors, faceshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowALLEigenface(dlpp_eigenvectors, lpp_eigenvectors, data_by_pca, eigen_vectors, faceshape):\n",
    "    # 显示前3个DLPP特征脸、前3个LPP特征脸、前3个PCA特征脸、前3个MLDA特征脸\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(12, 12))\n",
    "    \n",
    "    # 显示DLPP特征脸\n",
    "    for i in range(3):\n",
    "        axes[i, 0].imshow(dlpp_eigenvectors[:, i].reshape(faceshape), cmap=\"gray\")\n",
    "        axes[i, 0].set_title(\"DLPP Eigenface {}\".format(i+1))\n",
    "    \n",
    "    # 显示LPP特征脸\n",
    "    for i in range(3):\n",
    "        axes[i, 1].imshow(lpp_eigenvectors[:, i].reshape(faceshape), cmap=\"gray\")\n",
    "        axes[i, 1].set_title(\"LPP Eigenface {}\".format(i+1))\n",
    "    \n",
    "    # 显示PCA特征脸\n",
    "    for i in range(3):\n",
    "        axes[i, 2].imshow(data_by_pca[:, i].reshape(faceshape), cmap=\"gray\")\n",
    "        axes[i, 2].set_title(\"PCA Eigenface {}\".format(i+1))\n",
    "    \n",
    "    # 显示MLDA特征脸\n",
    "    for i in range(3):\n",
    "        axes[i, 3].imshow(eigen_vectors[:, i].reshape(faceshape), cmap=\"gray\")\n",
    "        axes[i, 3].set_title(\"MLDA Eigenface {}\".format(i+1))\n",
    "    \n",
    "    # 在下方的标题里显示d, k,t lpp_method等参数,标题字号为12并加粗\n",
    "    fig.suptitle(\"d={}, k={}, t={}, method={}, dataset=ORL\".format(d, k, t, lpp_method), fontsize=20, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "ShowALLEigenface(dlpp_eigenvectors, lpp_eigenvectors, data_by_pca, eigen_vectors,faceshape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
