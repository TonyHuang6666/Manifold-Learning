{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Preserving Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **1.The linear dimensionality reduction problem** <br>\n",
    "  The generic problem of linear dimensionality reduction is the following. Given a set $\\mathbf{x_1}, ..., \\mathbf{x_m} \\in \\mathbb{R^n}$, find a transformation matrix $A$ that maps these $m$ points to a set of points $\\mathbf{y_i}, ..., \\mathbf{y_m} \\in \\mathbb{R^l}$($l$<<$n$), such that $y_i$ \"represents\" $x_i$, where $y_i$ = $A_{}^{T} x_{i}$. Our method is of particular applicability in the special case where $\\mathbf{x_1}, ..., \\mathbf{x_m} \\in \\mathbb{M}$ and $\\mathbb{M}$ is a nonlinear manifold embedded in $\\mathbb{R^n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.The algorithm**<br>\n",
    " Locality Preserving Projection (LPP) is a linear approximation of the nonlinear Laplacian Eigenmap [2]. The algorithmic procedure is formally stated below:\n",
    " <br> <br>\n",
    " *1.Constructing the adjacency graph:* Let $G$ denote a graph with $m$ nodes. We put an edge between nodes $i$ and $j$ if $x_{i}$ and $x_{j}$ are \"close\". There are two variations: <br>\n",
    "*$(a)$*$\\mathcal{E}$-neighborhoods. [parameter $\\mathcal{E}$ $\\in \\mathbb{R}$ ] Nodes $i$ and $j$ are connected by an edge if <br>||$x_{i}$-$x_{j}||^2 $< $\\mathcal{E}$ where the norm is the usual Euclidean norm in ${R^n}$. <br>\n",
    "*$(b)$*$k$ nearest neighbors. [parameter $k$ $\\in \\mathbb{N}$ ] Nodes $i$ and $j$ are connected by an edge if $i$ is among $k$ nearest neighbors of $j$ or $j$ is among $k$ nearest neighbors of $i$.\n",
    " \n",
    " Note: The method of constructing an adjacency graph outlined above is correct\n",
    " if the data actually lie on a low dimensional manifold. In general, however, one\n",
    " might take a more utilitarian perspective and construct an adjacency graph based\n",
    " on any principle (for example, perceptual similarity for natural signals, hyperlink\n",
    " structures for web documents, etc.). Once such an adjacency graph is obtained,\n",
    " LPP will try to optimally preserve it in choosing projections.\n",
    " <br> <br> <br>\n",
    " *2.Choosing the weights:*  Here, as well, we have two variations for weighting the edges. $W$ is a sparse symmetric $m$ x $m$ matrix with $W_{ij}$ having the weight of the edge joining vertices $i$ and $j$, and 0 if there is no such edge.<br>\n",
    " $(a)$ Heat Kernel. [parameter $t$ $\\in \\mathbb{R}$]. If notes $i$ and $j$ are connected, put \n",
    "$$\n",
    "W_{ij} = e^{-\\frac{||x_i - x_j||^2}{t}}\n",
    "$$\n",
    " The justification for this choice of weights can be traced back to[2].\n",
    " <br>\n",
    " *$(b)$* Simple-minded.[No parameter]. $W_{ij} = 1$ if and only if vertices $i$ and $j$ are connected by an edge.\n",
    " <br> <br> <br>\n",
    " *3.Eigenmaps:* Compute the eigenvectors and eigenvalues for the generalized eigenvector problem:\n",
    " $$\n",
    "XLX^{T}a=\\mathcal{\\lambda}XDX^{T}a\n",
    " $$\n",
    " where $D$ is a diagonal matrix whose entries are column (or row, since $W$ is symmetric) sums of $W$, $D_{ii}=\\sum_j W_{ij}$. $L=D-W$ is the Laplacian matrix. The $i^{th}$ column of matrix $X$ is $x_i$.<br>\n",
    " Let the column vectors $\\mathbf{a_0}, ..., \\mathbf{a_{t-1}}$ be the solutions of equation above, ordered according to their eigenvalues, $\\mathbf{\\lambda_0} <...< \\mathbf{\\lambda_{t-1}}$. Thus, the embedding is as follows:\n",
    " $$\n",
    "x_i \\rarr x_j = A^Tx_i, A= (\\mathbf{a_0}, ..., \\mathbf{a_{t-1}})\n",
    " $$<br>\n",
    " where $y_i$ is a $l$-dimensional vector, and $A$ is a $n$ x $l$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3.Optimal Linear Embedding:*\n",
    "The following section is based on standard spectral graph theory.See [4] for a comprehensive reference and [2] for applications to data representation.<br>\n",
    "Recall that given a dataset we construct a weighted graph $G$ = ($V$,$E$) with edges connecting nearby points to each other. Consider the problem of mapping the weighted graph $G$ to a line so that connected points stay as close together as possible. Let $y$ = $(\\mathbf{y_1}, ..., \\mathbf{y_m})^T$ be such a map. A reasonable criterion for choosing a ”good” map is to minimize the following objective function [2]\n",
    "$$\n",
    "\\sum_{ij} (y_i-y_j)^2W_{ij}\n",
    "$$\n",
    "under appropriate constraints.The objective function with our choice of incurs a heavy penalty if neighboring points $x_i$ and $x_j$ are mapped far apart. Therefore, minimizing it is an attempt to ensure that if $x_i$ and $x_j$ are \"close\" then $y_i$ and $y_j$ are close as well.<br>\n",
    "Suppose $a$ is a transformation vector, that is, $y^T$ = $a^TX$, where the $i^{th}$ column vector of $X$ is $x_i$. By simple algebra formulation, the objective function can be reduced to\n",
    "$$\n",
    "\\frac{1}{2}\\sum_{ij} (y_i-y_j)^2 W_{ij}=\\frac{1}{2}\\sum_{ij}(a^Tx_i-a^Tx_j)^2 W_{ij}\n",
    "$$\n",
    "$$\n",
    "=\n",
    "\\sum_{i}a^T x_i D_{ii} x_i^T a- \\sum_{ij}a^T x_i W_{ij} x_i^T a\n",
    "$$\n",
    "$$\n",
    "=\n",
    "a^TX(D-W)X^Ta\n",
    "=\n",
    "a^TXLX^Ta\n",
    "$$\n",
    "where $X$ = $[\\mathbf{x_1}, ..., \\mathbf{x_m}]$, and $D$ is a diagonal matrix; its entries are column (or row, since $W$ is symmetric) sum of $W$, $D_{ii}=\\sum_{j}W_{ij}$. $L = D - W$ is the Laplacian matrix [4]. Matrix D provides a natural measure on the data points. The bigger the value $D_{ii}$(corresponding to $y_i$) is, the more \"important\" is $y_i$. Therefore, we impose a constraint as follows:\n",
    "$$\n",
    "y^TD_y = 1 \\rarr a^TXDX^Ta = 1\n",
    "$$\n",
    "Finally, the minimization problem reduces to finding:\n",
    "$$\n",
    "arg\\; min\\; a^TXLX^Ta\n",
    "$$\n",
    "$$\n",
    "a^TXDX^Ta=1\n",
    "$$\n",
    "\n",
    "The transformation vector $a$ that minimizes the objective function is given by the minimum eigenvalue solution to the generalized eigenvalue problem:\n",
    "\n",
    "$$\n",
    "XLX^{T}a=\\mathcal{\\lambda}XDX^{T}a\n",
    "$$\n",
    "It is easy to show that matrices $XLX^T$ and $XDX^T$ are symmetric and positive semidefinite. The vector $a_i(i = 0,2,...,l-1)$ that minimize the objective function are given by the minimum eigenvalue solutions to the generalized eigenvalue problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 局部保持投影"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们有一组来自未知概率分布的$n$维实向量的数据点集合。在机器学习和数据挖掘越来越多的学者感兴趣的情况下，也面临着数据维度非常大的情况。然而，有理由怀疑数据的“内在维度”要低得多。这启发我们思考降维的方法，让我们将数据表示在一个较低维的空间中。\n",
    "\n",
    "在本文中，我们提出了一种新的线性降维算法，称为局部保持投影（Locality Preserving Projections，LPP）。它构建了一个图，结合了数据集的邻域信息。利用图的拉普拉斯概念，我们计算出一个变换矩阵，将数据点映射到一个子空间。这种线性变换在某种意义上最优地保留了局部邻域信息。该算法生成的表示映射可以被视为是连续地从流形的几何中自然产生的线性离散近似<br>[2]M. Belkin and P. Niyogi, “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering ,” Advances in Neural Information Processing Systems 14,Vancouver, British Columbia, Canada, 2002.<br>新算法从多个角度都很有趣：\n",
    "<br>\n",
    "1. 这些映射旨在最小化与经典线性技术不同的目标标准。\n",
    "2. LPP 的局部保持特性可能特别适用于信息检索应用。如果希望在向量空间模型下检索音频、视频、文本文档，那么最终需要在低维空间进行最近邻搜索。由于 LPP 设计用于保持局部结构，因此低维空间中的最近邻搜索很可能产生与高维空间中相似的结果。这使得可以实现一个允许快速检索的索引方案。\n",
    "3. LPP 是线性的。这使得它快速且适用于实际应用。虽然有一些非线性技术具有上述特性（1）和（2），但我们不知道其他具有这种特性的线性投影技术。\n",
    "4. LPP 在任何地方都有定义。回想一下像 ISOMAP[6]、LLE[5]、Laplacian eigenmaps[2] 这样的非线性降维技术只定义在训练数据点上，不清楚如何对新的测试点评估映射。相比之下，局部保持投影可以简单地应用于任何新数据点，将其定位在降维表示空间中。\n",
    "5. LPP 可以在原始空间或数据点映射到的再生核希尔伯特空间（RKHS）中进行。这就产生了核LPP。\n",
    "\n",
    "由于所有这些特点，我们期望基于LPP的技术在探索性数据分析、信息检索和模式分类应用中成为主流技术，是主成分分析（PCA）技术的一种自然替代方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **1.线性降维问题** <br>\n",
    "给定一组 $\\mathbf{x_1}, ..., \\mathbf{x_m} \\in \\mathbb{R^n}$，找到一个转换矩阵 $A$，将这 $m$ 个点映射到一组点 $\\mathbf{y_i}, ..., \\mathbf{y_m} \\in \\mathbb{R^l}$（$l$<<$n$），使得 $y_i$ 能表示 $x_i$，其中 $y_i$ = $A_{}^{T} x_{i}$。我们的方法特别适用于以下特殊情况：$\\mathbf{x_1}, ..., \\mathbf{x_m} \\in \\mathbb{M}$，而 $\\mathbb{M}$ 是嵌入在 $\\mathbb{R^n}$ 中的非线性流形。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.算法**<br>\n",
    "局部保持投影（LPP）是非线性拉普拉斯特征映射的线性近似[2]。其算法步骤的正式陈述如下：\n",
    "\n",
    "*1. 构建邻接图：* 设 $G$ 为具有 $m$ 个节点的图。如果 $x_{i}$ 和 $x_{j}$ 是临近的的，则在节点 $i$ 和节点 $j$ 之间放置一条边。这一步有两种变体：<br>\n",
    "$(a)$ $\\mathcal{E}$-邻域。[参数 $\\mathcal{E}$ $\\in \\mathbb{R}$ ] 如果 $||x_{i}-x_{j}||^2 < \\mathcal{E}$，则节点 $i$ 和节点 $j$ 由一条边连接，其中范数是 $n$ 维欧几里得范数。<br>\n",
    "$(b)$ $k$ 最近邻。[参数 $k$ $\\in \\mathbb{N}$ ] 如果 $i$ 是 $j$ 的 $k$ 个最近邻节点之一，或者 $j$ 是 $i$ 的 $k$ 个最近邻节点之一，则节点 $i$ 和节点 $j$ 由一条边连接。\n",
    "\n",
    "注意： 上述构建邻接图的方法在数据实际上位于低维流形上时是正确的。然而，一般情况下，我们可能会更加倾向于实用主义，并基于任何原则（例如，自然信号的感知相似性、网页文档的超链接结构等）构建邻接图。一旦得到这样的邻接图，LPP 将尝试在选择投影时对其进行最佳保留。\n",
    "<br> <br> <br>\n",
    "*2. 权重选择：* 这里同样有两种边权重的变体。$W$ 是一个稀疏对称的 $m$ x $m$ 矩阵，其中 $W_{ij}$ 表示连接顶点 $i$ 和 $j$ 的边的权重，如果不存在这样的边则为 0。<br>\n",
    "$(a)$ 热核。[参数 $t$ $\\in \\mathbb{R}$] 如果节点 $i$ 和节点 $j$ 连接，则设定\n",
    " $$\n",
    "W_{ij} = e^{-\\frac{||x_i - x_j||^2}{t}}\n",
    " $$\n",
    "这种权重选择的理由可以追溯到[2]。<br>\n",
    "$(b)$ 简单方法。[无参数] 如果且仅如果顶点 $i$ 和 $j$ 由一条边连接，则 $W_{ij} = 1$。\n",
    "<br> <br> <br>\n",
    "*3. 特征映射：* 计算广义特征向量问题的特征向量和特征值：\n",
    "$$\n",
    "XLX^{T}a=\\mathcal{\\lambda}XDX^{T}a\n",
    "$$\n",
    "其中 $D$ 是一个对角矩阵，其条目是 $W$ 的列（或行，因为 $W$ 是对称的）和，$D_{ii}=\\sum_j W_{ij}$。$L=D-W$ 是拉普拉斯矩阵。矩阵 $X$ 的第 $i$ 列是 $x_i$。<br>\n",
    "令列向量 $\\mathbf{a_0}, ..., \\mathbf{a_{t-1}}$ 为上述方程的解，按照它们的特征值顺序排列，$\\mathbf{\\lambda_0} <...< \\mathbf{\\lambda_{t-1}}$。因此，嵌入如下所示：\n",
    " $$\n",
    "x_i \\rarr x_j = A^Tx_i, A= (\\mathbf{a_0}, ..., \\mathbf{a_{t-1}})\n",
    " $$<br>\n",
    " 其中 $y_i$ 是一个 $l$ 维向量，$A$ 是一个 $n$ x $l$ 的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 最优线性嵌入**<br>\n",
    "本节基于标准的谱图理论。参见 [4] 作为全面参考和 [2] 中数据表示的应用。\n",
    "\n",
    "回顾给定一个数据集，我们构建一个加权图 $G = (V, E)$，其中的边连接相邻的点。考虑将加权图 $G$ 映射到一条线上，使得相连接的点尽可能靠近。设 $y = (\\mathbf{y_1}, ..., \\mathbf{y_m})^T$ 为这样的映射。选择一个好的映射的合理标准是最小化以下目标函数 [2]\n",
    "$$\n",
    "\\sum_{ij} (y_i-y_j)^2W_{ij}\n",
    "$$\n",
    "在适当的约束条件下。我们的目标函数选择对于我们的权重选择而言，如果相邻点 $x_i$ 和 $x_j$ 被映射到较远的距离，则会产生很大的惩罚。因此，最小化它是试图确保如果 $x_i$ 和 $x_j$ 是接近的，那么 $y_i$ 和 $y_j$ 也是接近的。\n",
    "\n",
    "假设 $a$ 是一个转换向量，即 $y^T = a^TX$，其中 $X$ 的第 $i$ 列向量是 $x_i$。通过简单的代数表达，目标函数可以简化为\n",
    "$$\n",
    "\\frac{1}{2}\\sum_{ij} (y_i-y_j)^2 W_{ij}=\\frac{1}{2}\\sum_{ij}(a^Tx_i-a^Tx_j)^2 W_{ij}\n",
    "$$\n",
    "$$\n",
    "=\n",
    "\\sum_{i}a^T x_i D_{ii} x_i^T a- \\sum_{ij}a^T x_i W_{ij} x_i^T a\n",
    "$$\n",
    "$$\n",
    "=\n",
    "a^TX(D-W)X^Ta\n",
    "=\n",
    "a^TXLX^Ta\n",
    "$$\n",
    "其中 $X = [\\mathbf{x_1}, ..., \\mathbf{x_m}]$，$D$ 是一个对角矩阵；它的条目是 $W$ 的列（或行，因为 $W$ 是对称的）和，$D_{ii} = \\sum_{j}W_{ij}$。$L = D - W$ 是拉普拉斯矩阵 [4]。矩阵 $D$ 提供了数据点的一个自然度量。值 $D_{ii}$ 越大（对应于 $y_i$），$y_i$ 就越“重要”。因此，我们施加如下约束：\n",
    "$$\n",
    "y^TD_y = 1 \\rarr a^TXDX^Ta = 1\n",
    "$$\n",
    "最后，最小化问题简化为寻找：\n",
    "$$\n",
    "arg\\; min\\; a^TXLX^Ta\n",
    "$$\n",
    "$$\n",
    "a^TXDX^Ta=1\n",
    "$$\n",
    "\n",
    "使目标函数最小化的转换向量 $a$ 由广义特征值问题的最小特征值解决：\n",
    "\n",
    "$$\n",
    "XLX^{T}a=\\mathcal{\\lambda}XDX^{T}a\n",
    "$$\n",
    "很容易证明矩阵 $XLX^T$ 和 $XDX^T$ 都是对称且半正定的。最小化目标函数的向量 $a_i(i = 0,2,...,l-1)$ 由广义特征值问题的最小特征值解决。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
