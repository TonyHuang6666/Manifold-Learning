{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.linalg import eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef knn_graph(Data, method, k):\\n    # 获取样本点的数量\\n    n = Data.shape[0]\\n    print(\"样本点的数量：\", n)\\n    # 初始化k近邻图的邻接矩阵\\n    knn_adjacency_matrix = np.zeros((n, n))  \\n    # 计算欧式距离矩阵\\n    distances = np.sqrt(np.sum((Data[:, None] - Data) ** 2, axis=2))\\n    if method == \\'epsilon\\':\\n        return knn_adjacency_matrix, distances\\n    # 获取每个样本点的最近邻索引\\n    indices = np.argsort(distances, axis=1)[:, 1:k+1]\\n    # 构建k近邻图的权重矩阵\\n    # 遍历每个样本点的最近邻索引\\n    for i in range(n):\\n        knn_adjacency_matrix[i, indices[i]] = 1\\n        knn_adjacency_matrix[indices[i], i] = 1\\n    return knn_adjacency_matrix, distances\\n\\n# 以每个点到其他所有点的平均值作为每个数据点的平均邻域半径\\ndef compute_avg_radius(n, distances): \\n    radius = np.zeros(n) # 存储每个数据点的平均邻域半径\\n    for i in range(n): # 计算每个数据点的平均邻域半径\\n        avg_radius = np.mean(distances[i]) # 每个数据点到其他所有点的平均值\\n        radius[i] = avg_radius # 存储每个数据点的平均邻域半径\\n    return radius\\n\\n# 以将距离从小到大排序后的第k个距离作为每个数据点的半径\\ndef compute_k_maximun_radius(n, distances, k):\\n    radius = np.zeros(n)\\n    sorted_distances = np.sort(distances, axis=1)  # 对距离矩阵的每一行进行排序\\n    for i in range(n):\\n        # 选取第k个距离作为半径\\n        radius[i] = sorted_distances[i, k]\\n    return radius\\n\\n# 计算k最近邻接矩阵或epsilon邻接矩阵\\ndef compute_neighborhood_matrix(Data, method, k):\\n    n = len(Data)\\n    knn_adjacency_matrix, distances = knn_graph(Data, method, k)\\n    if method == \\'knn\\':\\n        return knn_adjacency_matrix, distances\\n    elif method == \\'epsilon\\':\\n        adjacency_matrix = np.zeros((n, n))\\n        radius = compute_k_maximun_radius(n, distances, k)  # 计算每个数据点的邻域半径\\n        for i in range(n):  # 对于数据集中的每个样本点 i\\n            neighbors = np.where(distances[i] <= radius[i])[0]  # 获取epsilon邻域内的样本索引\\n            adjacency_matrix[i, neighbors] = 1\\n            adjacency_matrix[neighbors, i] = 1\\n        return adjacency_matrix, distances\\n\\n# 构建基于热核方法的权重矩阵\\ndef construct_weight_matrix(Data, method, k, t):\\n    n = len(Data)\\n    print(\"样本点的数量：\", n)\\n    Weight_matrix = np.zeros((n, n))\\n    adjacency_matrix, distances = compute_neighborhood_matrix(Data, method, k)\\n    # 计算相似度矩阵\\n    similarity_matrix = np.exp(-distances ** 2 / t)\\n    # 将相似度矩阵按照邻接矩阵进行筛选，得到需要设置权重的位置\\n    i_indices, j_indices = np.where(adjacency_matrix == 1)\\n    # 设置权重\\n    Weight_matrix[i_indices, j_indices] = similarity_matrix[i_indices, j_indices]\\n    Weight_matrix[j_indices, i_indices] = similarity_matrix[i_indices, j_indices]  # 对称矩阵\\n    # 计算全局相似度\\n    \\'\\'\\'\\n    修正权重矩阵的原理是利用全局相似度来修正局部相似度得到的权重矩阵，以使得整个权重矩阵更加平滑和连续。\\n    \\'\\'\\'\\n    Weight_matrix += np.exp(-distances ** 2 / t)\\n    return Weight_matrix   \\n\\n\\ndef LPP(Data, d, method, k, t):\\n    # Step 1: 计算权重矩阵\\n    Weight_matrix = construct_weight_matrix(Data, method, k, t)\\n    print(\"权重矩阵形状：\", Weight_matrix.shape)\\n    # Step 2: 计算度矩阵和拉普拉斯矩阵\\n    Degree_matrix = np.diag(np.sum(Weight_matrix, axis=1))\\n    print(\"度矩阵形状：\", Degree_matrix.shape)\\n    Laplacian_matrix = Degree_matrix - Weight_matrix\\n    print(\"拉普拉斯矩阵形状：\", Laplacian_matrix.shape)\\n    # Step 3: 进行特征映射\\n    eigenvalues, eigenvectors = eigs(Laplacian_matrix, k=d+1, which=\\'SR\\')\\n    sorted_indices = np.argsort(eigenvalues.real)\\n    selected_indices = sorted_indices[1:d + 1]\\n    selected_eigenvectors = eigenvectors.real[:, selected_indices]\\n    return selected_eigenvectors\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def knn_graph(Data, method, k):\n",
    "    # 获取样本点的数量\n",
    "    n = Data.shape[0]\n",
    "    print(\"样本点的数量：\", n)\n",
    "    # 初始化k近邻图的邻接矩阵\n",
    "    knn_adjacency_matrix = np.zeros((n, n))  \n",
    "    # 计算欧式距离矩阵\n",
    "    distances = np.sqrt(np.sum((Data[:, None] - Data) ** 2, axis=2))\n",
    "    if method == 'epsilon':\n",
    "        return knn_adjacency_matrix, distances\n",
    "    # 获取每个样本点的最近邻索引\n",
    "    indices = np.argsort(distances, axis=1)[:, 1:k+1]\n",
    "    # 构建k近邻图的权重矩阵\n",
    "    # 遍历每个样本点的最近邻索引\n",
    "    for i in range(n):\n",
    "        knn_adjacency_matrix[i, indices[i]] = 1\n",
    "        knn_adjacency_matrix[indices[i], i] = 1\n",
    "    return knn_adjacency_matrix, distances\n",
    "\n",
    "# 以每个点到其他所有点的平均值作为每个数据点的平均邻域半径\n",
    "def compute_avg_radius(n, distances): \n",
    "    radius = np.zeros(n) # 存储每个数据点的平均邻域半径\n",
    "    for i in range(n): # 计算每个数据点的平均邻域半径\n",
    "        avg_radius = np.mean(distances[i]) # 每个数据点到其他所有点的平均值\n",
    "        radius[i] = avg_radius # 存储每个数据点的平均邻域半径\n",
    "    return radius\n",
    "\n",
    "# 以将距离从小到大排序后的第k个距离作为每个数据点的半径\n",
    "def compute_k_maximun_radius(n, distances, k):\n",
    "    radius = np.zeros(n)\n",
    "    sorted_distances = np.sort(distances, axis=1)  # 对距离矩阵的每一行进行排序\n",
    "    for i in range(n):\n",
    "        # 选取第k个距离作为半径\n",
    "        radius[i] = sorted_distances[i, k]\n",
    "    return radius\n",
    "\n",
    "# 计算k最近邻接矩阵或epsilon邻接矩阵\n",
    "def compute_neighborhood_matrix(Data, method, k):\n",
    "    n = len(Data)\n",
    "    knn_adjacency_matrix, distances = knn_graph(Data, method, k)\n",
    "    if method == 'knn':\n",
    "        return knn_adjacency_matrix, distances\n",
    "    elif method == 'epsilon':\n",
    "        adjacency_matrix = np.zeros((n, n))\n",
    "        radius = compute_k_maximun_radius(n, distances, k)  # 计算每个数据点的邻域半径\n",
    "        for i in range(n):  # 对于数据集中的每个样本点 i\n",
    "            neighbors = np.where(distances[i] <= radius[i])[0]  # 获取epsilon邻域内的样本索引\n",
    "            adjacency_matrix[i, neighbors] = 1\n",
    "            adjacency_matrix[neighbors, i] = 1\n",
    "        return adjacency_matrix, distances\n",
    "\n",
    "# 构建基于热核方法的权重矩阵\n",
    "def construct_weight_matrix(Data, method, k, t):\n",
    "    n = len(Data)\n",
    "    print(\"样本点的数量：\", n)\n",
    "    Weight_matrix = np.zeros((n, n))\n",
    "    adjacency_matrix, distances = compute_neighborhood_matrix(Data, method, k)\n",
    "    # 计算相似度矩阵\n",
    "    similarity_matrix = np.exp(-distances ** 2 / t)\n",
    "    # 将相似度矩阵按照邻接矩阵进行筛选，得到需要设置权重的位置\n",
    "    i_indices, j_indices = np.where(adjacency_matrix == 1)\n",
    "    # 设置权重\n",
    "    Weight_matrix[i_indices, j_indices] = similarity_matrix[i_indices, j_indices]\n",
    "    Weight_matrix[j_indices, i_indices] = similarity_matrix[i_indices, j_indices]  # 对称矩阵\n",
    "    # 计算全局相似度\n",
    "    '''\n",
    "    修正权重矩阵的原理是利用全局相似度来修正局部相似度得到的权重矩阵，以使得整个权重矩阵更加平滑和连续。\n",
    "    '''\n",
    "    Weight_matrix += np.exp(-distances ** 2 / t)\n",
    "    return Weight_matrix   \n",
    "\n",
    "\n",
    "def LPP(Data, d, method, k, t):\n",
    "    # Step 1: 计算权重矩阵\n",
    "    Weight_matrix = construct_weight_matrix(Data, method, k, t)\n",
    "    print(\"权重矩阵形状：\", Weight_matrix.shape)\n",
    "    # Step 2: 计算度矩阵和拉普拉斯矩阵\n",
    "    Degree_matrix = np.diag(np.sum(Weight_matrix, axis=1))\n",
    "    print(\"度矩阵形状：\", Degree_matrix.shape)\n",
    "    Laplacian_matrix = Degree_matrix - Weight_matrix\n",
    "    print(\"拉普拉斯矩阵形状：\", Laplacian_matrix.shape)\n",
    "    # Step 3: 进行特征映射\n",
    "    eigenvalues, eigenvectors = eigs(Laplacian_matrix, k=d+1, which='SR')\n",
    "    sorted_indices = np.argsort(eigenvalues.real)\n",
    "    selected_indices = sorted_indices[1:d + 1]\n",
    "    selected_eigenvectors = eigenvectors.real[:, selected_indices]\n",
    "    return selected_eigenvectors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adaptive_k(Data, threshold):\n",
    "    n = Data.shape[1]  # 修改此处获取样本点的数量\n",
    "    distances = np.sqrt(np.sum((Data.T[:, :, None] - Data.T[:, :, None].T) ** 2, axis=1)) \n",
    "    #np.savetxt('distances.csv', distances, delimiter=',')  # 保存距离矩阵\n",
    "    sorted_distances = np.sort(distances, axis=1)  # 对距离矩阵的每一行进行排序\n",
    "    #np.savetxt('sorted_distances.csv', sorted_distances, delimiter=',')  # 保存排序后的距离矩阵\n",
    "    # 找出sorted_distances每一行的前一列与后一列距离之差第一次小于阈值的位置并保存为一个列矩阵\n",
    "    adaptive_k = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        for j in range(1, n-1):\n",
    "            if (sorted_distances[i, j+1] - sorted_distances[i, j])  < threshold:\n",
    "                adaptive_k[i] = j\n",
    "                break           \n",
    "    #np.savetxt('adaptive_k.csv', adaptive_k, delimiter=',')  # 保存index\n",
    "    return adaptive_k, sorted_distances, distances\n",
    "\n",
    "# 根据adaptive_k中每一行的k值和sorted_distances对每个数据点构建knn graph\n",
    "def adaptive_knn_graph(Data, threshold):\n",
    "    adaptive_k, sorted_distances, distances = compute_adaptive_k(Data, threshold)\n",
    "    n = Data.shape[1]  # 修改此处获取样本点的数量\n",
    "    knn_adjacency_matrix = np.zeros((n, n))  \n",
    "    for i in range(n):\n",
    "        indices = np.argsort(sorted_distances[i])[:int(adaptive_k[i])]\n",
    "        knn_adjacency_matrix[i, indices] = 1\n",
    "        knn_adjacency_matrix[indices, i] = 1\n",
    "    return knn_adjacency_matrix, adaptive_k, sorted_distances, distances \n",
    "\n",
    "def compute_avg_radius(n, distances): \n",
    "    radius = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        avg_radius = np.mean(distances[:, i])  # 修改计算每个数据点的平均邻域半径的方式\n",
    "        radius[i] = avg_radius\n",
    "    return radius\n",
    "\n",
    "def compute_knn_average_radius(distances, k):\n",
    "    sorted_distances = np.sort(distances, axis=1)  # 对距离矩阵的每一行进行排序\n",
    "    avg_knn_distances = np.mean(sorted_distances[:, 1:k+1], axis=1)  # 计算每个数据点的前k个距离的平均值作为半径\n",
    "    return avg_knn_distances\n",
    "\n",
    "def compute_neighborhood_matrix(Data, method, threshold):\n",
    "    n = Data.shape[1]  # 修改获取样本点的数量的方式\n",
    "    knn_adjacency_matrix, adaptive_k, sorted_distances, distances = adaptive_knn_graph(Data, threshold)\n",
    "    if method == 'knn':\n",
    "        return knn_adjacency_matrix, distances\n",
    "    adjacency_matrix = np.zeros((n, n))\n",
    "    k_mean = int(np.mean(adaptive_k))  # 计算k的均值\n",
    "    radius = compute_knn_average_radius(distances, k_mean)\n",
    "    for i in range(n):\n",
    "        neighbors = np.where(distances[:, i] <= radius[i])[0]  # 修改获取epsilon邻域内的样本索引的方式\n",
    "        adjacency_matrix[i, neighbors] = 1\n",
    "        adjacency_matrix[neighbors, i] = 1\n",
    "    return adjacency_matrix, distances\n",
    "\n",
    "def construct_weight_matrix(Data, method, threshold, t):\n",
    "    n = Data.shape[1]  # 修改获取样本点的数量的方式\n",
    "    Weight_matrix = np.zeros((n, n))\n",
    "    adjacency_matrix, distances = compute_neighborhood_matrix(Data, method, threshold)\n",
    "    similarity_matrix = np.exp(-distances ** 2 / t)\n",
    "    i_indices, j_indices = np.where(adjacency_matrix == 1)\n",
    "    Weight_matrix[i_indices, j_indices] = similarity_matrix[i_indices, j_indices]\n",
    "    Weight_matrix[j_indices, i_indices] = similarity_matrix[i_indices, j_indices]\n",
    "    Weight_matrix += np.exp(-distances ** 2 / t)\n",
    "    return Weight_matrix\n",
    "\n",
    "def LPP(Data, d, method, threshold, t):\n",
    "    Weight_matrix = construct_weight_matrix(Data, method, threshold, t)\n",
    "    Degree_matrix = np.diag(np.sum(Weight_matrix, axis=1))\n",
    "    Laplacian_matrix = Degree_matrix - Weight_matrix\n",
    "    objective_value = np.dot(np.dot(Data, Laplacian_matrix), Data.T)  # 计算目标函数\n",
    "    eigenvalues, eigenvectors = eigs(objective_value, k=d+1)\n",
    "    sorted_indices = np.argsort(eigenvalues.real)\n",
    "    selected_indices = sorted_indices[1:d + 1]\n",
    "    selected_eigenvectors = eigenvectors.real[:, selected_indices]\n",
    "    return selected_eigenvectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "def read_images(dataset_dir, target_size=(32, 32)):\n",
    "    data = []  # 存储图像数据的列表\n",
    "    labels = []  # 存储标签的列表\n",
    "    faceshape = [] # 存储图像形状\n",
    "    for class_dir in os.listdir(dataset_dir):  # 遍历数据集文件夹中的文件夹（每个文件夹代表一个类别）\n",
    "        class_path = os.path.join(dataset_dir, class_dir)  # 类别文件夹路径\n",
    "        for file_name in os.listdir(class_path):  # 遍历每个类别文件夹中的图像文件\n",
    "            file_path = os.path.join(class_path, file_name)  # 图像文件路径\n",
    "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # 读取灰度图像\n",
    "            # 缩放图像至目标尺寸\n",
    "            img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "            # 读取第一张灰度图像的大小作为图片形状\n",
    "            faceshape = img.shape\n",
    "            data.append(img.flatten())  # 将图像展平并添加到数据列表中\n",
    "            labels.append(int(class_dir))  # 将类别标签添加到标签列表中\n",
    "    return np.array(data), np.array(labels).reshape(-1, 1), faceshape  # 返回图像数据和标签\n",
    "\n",
    "# 训练集和测试集划分\n",
    "def train_test_split(data, labels, train_test_split_ratio):\n",
    "    num_samples = data.shape[0]  # 总样本数\n",
    "    train_samples = int(num_samples * train_test_split_ratio)  # 训练集样本数\n",
    "    \n",
    "    # 洗牌算法打乱数据集\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    train_data = data[:train_samples]\n",
    "    train_labels = labels[:train_samples]\n",
    "    test_data = data[train_samples:]\n",
    "    test_labels = labels[train_samples:]\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowEigenface(eigenfaces, faceshape):\n",
    "    # 显示前16个特征脸\n",
    "    fig, axes = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(8, 10))\n",
    "    for i in range(16):\n",
    "        axes[i%4][i//4].imshow(eigenfaces[:, i].reshape(faceshape), cmap=\"gray\")\n",
    "    #print(\"显示特征脸\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人脸图像形状: (32, 32)\n",
      "类别数量: 400\n",
      "图像数量: 400\n"
     ]
    }
   ],
   "source": [
    "# 读取人脸图像\n",
    "faces, classes, faceshape = read_images(\"ORL\")  # 读取图像数据和标签\n",
    "\n",
    "# 打印一些细节\n",
    "print(\"人脸图像形状:\", faceshape)\n",
    "print(\"类别数量:\", len(classes))\n",
    "print(\"图像数量:\", len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 应用LPP并选择前K个主成分作为特征脸\\n\\nd = 70\\nthreshold = 0.05\\nt = 100000\\nmethod = \\'knn\\'\\n\\ntrain_data = train_data.T\\nprint(\"训练集形状：\", train_data.shape)\\n\\noverall_mean = np.mean(train_data , axis=1).reshape(-1, 1)\\nprint(\"平均人脸形状:\", overall_mean.shape)\\n\\n#既是特征脸，又是transformation matrix A\\nlpp_eigenfaces = LPP(train_data, d, method, threshold, t)\\nprint(\"特征脸形状:\", lpp_eigenfaces.shape)\\n\\n#既是权重矩阵，又是Y\\nlpp_weight_matrix = lpp_eigenfaces.T @ (train_data-overall_mean) \\nprint(\"权重矩阵形状:\", lpp_weight_matrix.shape)\\n\\nShowEigenface(lpp_eigenfaces, faceshape)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 应用LPP并选择前K个主成分作为特征脸\n",
    "\n",
    "d = 70\n",
    "threshold = 0.05\n",
    "t = 100000\n",
    "method = 'knn'\n",
    "\n",
    "train_data = train_data.T\n",
    "print(\"训练集形状：\", train_data.shape)\n",
    "\n",
    "overall_mean = np.mean(train_data , axis=1).reshape(-1, 1)\n",
    "print(\"平均人脸形状:\", overall_mean.shape)\n",
    "\n",
    "#既是特征脸，又是transformation matrix A\n",
    "lpp_eigenfaces = LPP(train_data, d, method, threshold, t)\n",
    "print(\"特征脸形状:\", lpp_eigenfaces.shape)\n",
    "\n",
    "#既是权重矩阵，又是Y\n",
    "lpp_weight_matrix = lpp_eigenfaces.T @ (train_data-overall_mean) \n",
    "print(\"权重矩阵形状:\", lpp_weight_matrix.shape)\n",
    "\n",
    "ShowEigenface(lpp_eigenfaces, faceshape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(i, faceshape, overall_mean, train_labels, test_labels, train_data, test_data, lpp_eigenfaces, lpp_weight_matrix):\n",
    "    # 计算测试图像的权重向量\n",
    "    query = test_data[i]\n",
    "    #print(\"测试图像形状:\", query.shape)\n",
    "    #print(\"平均人脸形状:\", overall_mean.shape)\n",
    "    #print(\"特征脸形状:\", lpp_eigenfaces.shape)\n",
    "    query_weight = (lpp_eigenfaces.T @ (query - overall_mean.flatten()).reshape(-1, 1))\n",
    "    # 计算测试图像与数据集中每个人脸的欧氏距离\n",
    "    euclidean_distances = np.linalg.norm(lpp_weight_matrix - query_weight, axis=0)\n",
    "    # 找到最佳匹配的人脸\n",
    "    best_match_index = np.argmin(euclidean_distances)\n",
    "    #判断是否匹配正确\n",
    "    flag = False\n",
    "    if train_labels[best_match_index] == test_labels[i]:\n",
    "        flag = True\n",
    "    else:\n",
    "        flag = False\n",
    "    \"\"\"\n",
    "    # 可视化\n",
    "    fig, axes = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 6))\n",
    "    axes[0].imshow(query.reshape(faceshape), cmap=\"gray\")\n",
    "    axes[0].set_title(\"Query Image\")\n",
    "    axes[1].set_xlabel(\"Euclidean Distance: {:.0f}\".format(euclidean_distances[best_match_index]))\n",
    "    axes[1].imshow(train_data[best_match_index].reshape(faceshape), cmap=\"gray\")\n",
    "    axes[1].set_title(\"Best Match\")\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    return flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tony\\AppData\\Local\\Temp\\ipykernel_15940\\105962115.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  indices = np.argsort(sorted_distances[i])[:int(adaptive_k[i])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recognition Rate: 0.9582499999999999\n",
      "[0.9166666666666666, 0.95, 0.9333333333333333, 0.9666666666666667, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9666666666666667, 0.9833333333333333, 0.9833333333333333, 0.975, 0.975, 0.95, 0.925, 0.975, 0.975, 0.975, 0.975, 0.95, 0.975, 0.9333333333333333, 0.9583333333333334, 0.9333333333333333, 0.975, 0.95, 0.9833333333333333, 0.9666666666666667, 0.95, 0.9333333333333333, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.975, 0.9666666666666667, 0.9583333333333334, 0.9916666666666667, 0.9083333333333333, 0.9416666666666667, 0.9583333333333334, 0.925, 0.975, 0.95, 0.9666666666666667, 0.9833333333333333, 0.9833333333333333, 0.95, 0.9583333333333334, 0.9833333333333333, 0.9583333333333334, 0.9833333333333333, 0.9416666666666667, 0.9833333333333333, 0.9666666666666667, 0.95, 0.925, 0.925, 0.95, 0.975, 0.975, 0.925, 0.95, 0.975, 0.9833333333333333, 0.9333333333333333, 0.9166666666666666, 0.9083333333333333, 0.9583333333333334, 0.9583333333333334, 0.9333333333333333, 0.925, 0.9833333333333333, 0.975, 0.975, 0.9916666666666667, 0.9666666666666667, 0.95, 0.95, 0.975, 0.9166666666666666, 0.95, 0.9166666666666666, 0.9666666666666667, 0.9416666666666667, 0.9833333333333333, 0.9666666666666667, 0.9166666666666666, 0.9083333333333333, 0.9666666666666667, 0.9416666666666667, 0.95, 0.95, 0.9666666666666667, 0.9666666666666667, 0.9833333333333333, 0.9666666666666667, 0.9666666666666667, 0.9416666666666667, 0.9666666666666667, 0.9833333333333333, 0.9833333333333333]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_data, train_labels, test_data, test_labels = train_test_split(faces, classes, 0.7)  # 划分训练集和测试集\n",
    "d = 70\n",
    "threshold = 0.5\n",
    "t = 100000\n",
    "method = 'knn'\n",
    "train_data = train_data.T\n",
    "overall_mean = np.mean(train_data , axis=1).reshape(-1, 1)\n",
    "lpp_eigenfaces = LPP(train_data, d, method, threshold, t)\n",
    "lpp_weight_matrix = lpp_eigenfaces.T @ (train_data-overall_mean)\n",
    "\n",
    "# 单次识别率统计\n",
    "wrong_times = 0\n",
    "right_times = 0\n",
    "for i in range(test_data.shape[0]):\n",
    "    flag = test_image(i, faceshape, overall_mean, train_labels, test_labels, train_data, test_data, lpp_eigenfaces, lpp_weight_matrix)\n",
    "    if flag:\n",
    "                right_times += 1\n",
    "    else:\n",
    "        wrong_times += 1\n",
    "rate = right_times / test_data.shape[0]\n",
    "print(f\"Recognition Rate: {rate}\")\n",
    "#ShowEigenface(lpp_eigenfaces, faceshape)\n",
    "\"\"\"\n",
    "\n",
    "# 求平均识别率\n",
    "times = 100\n",
    "rates = []\n",
    "for i in range(times):\n",
    "    d = 70\n",
    "    threshold = 0.5\n",
    "    t = 100000\n",
    "    method = 'knn'\n",
    "    train_data, train_labels, test_data, test_labels = train_test_split(faces, classes, 0.7)  # 划分训练集和测试集\n",
    "    train_data = train_data.T\n",
    "    overall_mean = np.mean(train_data , axis=1).reshape(-1, 1)\n",
    "    lpp_eigenfaces = LPP(train_data, d, method, threshold, t)\n",
    "    lpp_weight_matrix = lpp_eigenfaces.T @ (train_data-overall_mean)\n",
    "    wrong_times = 0\n",
    "    right_times = 0\n",
    "    for i in range(test_data.shape[0]):\n",
    "        flag = test_image(i, faceshape, overall_mean, train_labels, test_labels, train_data, test_data, lpp_eigenfaces, lpp_weight_matrix)\n",
    "        if flag:\n",
    "                    right_times += 1\n",
    "        else:\n",
    "            wrong_times += 1\n",
    "    rate = right_times / test_data.shape[0]\n",
    "    rates.append(rate)\n",
    "print(f\"Average Recognition Rate: {np.mean(rates)}\")\n",
    "print(rates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
