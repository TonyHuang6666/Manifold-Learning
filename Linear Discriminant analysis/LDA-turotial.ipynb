{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA tries to find the subspace that best discriminates different face classes by maximizing the between-class scatter matirx $S_b$, while minimizing the within-class scatter matrix $S_w$ in the projective subspace. $S_w$ and $S_b$ are defined as follows:\n",
    "$$\n",
    "S_w = \\sum_{i=1}^C \\sum_{x_k \\in X_i} (x_k - m_i)(x_k - m_i)^T\n",
    "$$\n",
    "$$\n",
    "S_b = \\sum_{i=1}^C n_i (m_i - m)(m_i - m)^T\n",
    "$$\n",
    "<br>where $m_i$ is the mean face for the individual class $X_i$ and $n_i$ is the number of samples in class $X_i$.\n",
    "<br>LDA subspace is spanned by a set of vectors $W_{LDA}$, satisfying \n",
    "$$\n",
    "W_{LDA} = argmax |\\frac {W_{LDA}^T S_b W_{LDA}} {W_{LDA}^T S_w W_{LDA}}|\n",
    "$$\n",
    "<br>For recognition, the linear discriminant function is defined as:\n",
    "$$\n",
    "d(Q) = W_{LDA}^T (Q-P)\n",
    "$$\n",
    "<br>The face class is chosen to minimize ||d||."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA subspace is derived from $S_w$ and $S_b$. Similar to PCA, we can also study the LDA subspace using image difference. For simplicity, we assume that each class has the same sample number $n$. Similar to PCA, we have:\n",
    "$$\n",
    "S_w = \\sum_{i=1}^C \\sum_{x_k \\in X_i} (x_k - m_i)(x_k - m_i)^T\n",
    "$$\n",
    "$$\n",
    "=\\frac {1} {2n} \\sum_{i=1}^C \\sum_{x_{k1},x_{k2} \\in X_i} (x_{k1} - x_{k2})(x_{k1} - x_{k2})^T\n",
    "$$\n",
    "$$\n",
    "S_b = \\sum_{i=1}^C n(m_i - m)(m_i - m)^T\n",
    "$$\n",
    "$$\n",
    "=\\frac {n} {2C} \\sum_{i=1}^C \\sum_{j=1}^C n(m_i - m_j)(m_i - m_j)^T\n",
    "$$\n",
    "<br>$S_w$ is the covariance matrix of the intrapersonal subspace, which characterizes the distribution of face variation for the same individual. Using the mean face image to describe each individual class, $S_b$ characterizes the distribution of the difference between any two mean face images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性判别分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA 试图找到最能区分不同面部类别的子空间，通过在投影子空间中最大化类间散布矩阵 $S_b$，同时最小化类内散布矩阵 $S_w$。$S_w$ 和 $S_b$ 定义如下： $$ S_w = \\sum_{i=1}^C \\sum_{x_k \\in X_i} (x_k - m_i)(x_k - m_i)^T $$ $$ S_b = \\sum_{i=1}^C n_i (m_i - m)(m_i - m)^T $$ 其中 $m_i$ 是个体类别 $X_i$ 的平均脸，$n_i$ 是类别 $X_i$ 中的样本数量。 LDA 子空间由一组向量 $W_{LDA}$ 张成，满足 $$ W_{LDA} = argmax |\\frac {W_{LDA}^T S_b W_{LDA}} {W_{LDA}^T S_w W_{LDA}}| $$ 对于识别，线性判别函数定义为： $$ d(Q) = W_{LDA}^T (Q-P) $$ 选择面部类别使 ||d|| 最小化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA 子空间是从 $S_w$ 和 $S_b$ 导出的。与 PCA 类似，我们也可以使用图像差异来研究 LDA 子空间。为了简单起见，我们假设每个类别的样本数量 $n$ 是相同的。与 PCA 类似，我们有： $$ S_w = \\sum_{i=1}^C \\sum_{x_k \\in X_i} (x_k - m_i)(x_k - m_i)^T $$ $$ =\\frac {1} {2n} \\sum_{i=1}^C \\sum_{x_{k1},x_{k2} \\in X_i} (x_{k1} - x_{k2})(x_{k1} - x_{k2})^T $$ $$ S_b = \\sum_{i=1}^C n(m_i - m)(m_i - m)^T $$ $$ =\\frac {n} {2C} \\sum_{i=1}^C \\sum_{j=1}^C n(m_i - m_j)(m_i - m_j)^T $$ $S_w$ 是个体子空间的协方差矩阵，它描述了同一个体的面部变化的分布。使用平均面部图像来描述每个个体类别，$S_b$ 描述了任意两个平均面部图像之间的差异的分布。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
